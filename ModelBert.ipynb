{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelBert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHwH0PoJMee5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Downgrading tensorflow in order to make BERT processing compatible\n",
        "pip install tensorflow==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgG2GTXPNDWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBURTlWZNGEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import email\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import metrics \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "%tensorflow_version 1.1.0\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ITljSPNXYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading and preprocessing dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/ALDA_Project/emails.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW1YQwbwNaI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_sent_mails = dataset[dataset['file'].str.contains('sent')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWDYmJOpNdbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_sent_mails = dataset_sent_mails.assign(sender=dataset_sent_mails[\"file\"].map(lambda x: re.search(\"(.*)/.*sent\", x).group(1)).values)\n",
        "dataset_sent_mails.drop(\"file\", axis=1, inplace=True)\n",
        "#print(dataset_sent_mails.head(5))\n",
        "print(dataset_sent_mails[\"sender\"].value_counts().head(15))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T9zsnksNhm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def email_preprocessing(email_message):\n",
        "    msg = email.message_from_string(email_message)\n",
        "    \n",
        "    email_content = []\n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            email_content.append(part.get_payload())\n",
        "            \n",
        "    result = {}\n",
        "    for key in msg.keys(): \n",
        "        result[key] = msg[key]\n",
        "    result[\"content\"] = ''.join(email_content)\n",
        "    # msg[\"content\"] = ''.join(email_content)\n",
        "    return result\n",
        "\n",
        "def content_preprocessing(content):\n",
        "    content = re.sub(\"[^a-zA-Z]\",\" \", content)\n",
        "    words = content.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stops]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrFi7SfBdmtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = pd.DataFrame(list(map(email_preprocessing, sent_user_dataset.message)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQQCyxYQNmdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for performing classification of all users\n",
        "users = dataset_sent_mails[\"sender\"].value_counts().index.values\n",
        "mapping = {}\n",
        "for i, user in enumerate(users):\n",
        "  \n",
        "  mapping[user] = i\n",
        "sent_user_dataset = dataset_sent_mails\n",
        "final_data = pd.DataFrame(list(map(email_preprocessing, sent_user_dataset.message)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMzYQZe0NzKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = pd.DataFrame(list(map(content_preprocessing, final_data[['Subject', 'content']].apply(lambda x: ' '.join(x), axis=1))), columns = [\"content\"])\n",
        "# final_data.head()\n",
        "final_data = final_data.assign(user_number= sent_user_dataset[\"sender\"].values)\n",
        "final_data = final_data.replace({'user_number': mapping})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlvjQWpHN2nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the required bert model\n",
        "#Implemented Google's Pretrained Bert Model for email classification and vector generation\n",
        "#Model Used: Bert-'uncased_L-12_H-768_A-12'\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36tkQ1w4OAwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing BERT dependencies\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abwc7HDLPOsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3up1BHnYOJmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting the dataset\n",
        "X = final_data.content.values\n",
        "y = final_data.user_number.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "#Function to generate data for the BERT model\n",
        "def create_examples(lines, set_type, labels=None):\n",
        "\n",
        "    guid = f'{set_type}'\n",
        "    examples = []\n",
        "    if guid == 'train':\n",
        "        for line, label in zip(lines, labels):\n",
        "            text_a = line\n",
        "            label = str(label)\n",
        "            examples.append(\n",
        "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    else:\n",
        "        for line in lines:\n",
        "            text_a = line\n",
        "            label = '0'\n",
        "            examples.append(\n",
        "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    return examples\n",
        "\n",
        "\n",
        "folder = 'model_folder'\n",
        "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD-DK1-4OZMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Specification\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "BERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\n",
        "OUTPUT_DIR = f'{folder}/outputs'\n",
        "print(f'>> Model output directory: {OUTPUT_DIR}')\n",
        "print(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IvKslqrPWyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Hyper Parameters\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "EVAL_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-2\n",
        "NUM_TRAIN_EPOCHS = 5.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "MAX_SEQ_LENGTH = 100\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 100000 \n",
        "ITERATIONS_PER_LOOP = 100000\n",
        "NUM_TPU_CORES = 8\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "\n",
        "label_list = [str(num) for num in range(155)]\n",
        "# label_list = [str(num) for num in range(15)]\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
        "train_examples = create_examples(X_train, 'train', labels=y_train)\n",
        "\n",
        "tpu_cluster_resolver = None \n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMdUwOUnPdnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model training\n",
        "train_features = run_classifier.convert_examples_to_features(\n",
        "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "print('>> Started training at {} '.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(train_examples)))\n",
        "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E0AFFXgbOYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L67qwUTHPlJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('>> Finished training at {}'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMY779YmvAQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for creating input closure to be passed to TPUEstimator\n",
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "#Actual input function\n",
        "  def input_fn(params):\n",
        "    print(params)\n",
        "    batch_size = 500\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvdWwUOuvL0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting the test dataset\n",
        "predict_examples = create_examples(X_test_temp, 'test')\n",
        "# print(predict_examples)\n",
        "predict_features = run_classifier.convert_examples_to_features(\n",
        "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "predict_input_fn = input_fn_builder(\n",
        "    features=predict_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "result = estimator.predict(input_fn=predict_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI-QTgupwF5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "for prediction in result:\n",
        "      preds.append(np.argmax(prediction['probabilities']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8ngnzQlxNV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}