{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "LSTM_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxlIaWQRsj5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02KyqEKfsq_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/Alda_project/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdmDkLCEvvYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import Input, Dense, Dropout, LSTM, Bidirectional\n",
        "from keras import Model\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import email\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import metrics \n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbJd_21b6l6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX2PLjcXNbog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/My Drive/Alda_project/emails.csv')\n",
        "dataset_sent_mails = dataset[dataset['file'].str.contains('sent')]\n",
        "dataset_sent_mails = dataset_sent_mails.assign(sender=dataset_sent_mails[\"file\"].map(lambda x: re.search(\"(.*)/.*sent\", x).group(1)).values)\n",
        "dataset_sent_mails.drop(\"file\", axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HJPkB3UNcre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "users = dataset_sent_mails[\"sender\"].value_counts().head(15).index.values # extract top 15 users\n",
        "mapping = {}\n",
        "for i, user in enumerate(users, start = 1):\n",
        "  mapping[user] = i\n",
        "sent_user_dataset = dataset_sent_mails[dataset_sent_mails.sender.isin(users)] # extracted data of 15 users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGp-D3hQNdPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing email content\n",
        "def email_preprocessing(email_message):\n",
        "    msg = email.message_from_string(email_message)\n",
        "    \n",
        "    email_content = []\n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            email_content.append(part.get_payload())\n",
        "            \n",
        "    result = {}\n",
        "    for key in msg.keys(): \n",
        "        result[key] = msg[key]\n",
        "    result[\"content\"] = ''.join(email_content)\n",
        "    # msg[\"content\"] = ''.join(email_content)\n",
        "    return result\n",
        "\n",
        "#Function for preprocessing of text data\n",
        "def content_preprocessing(content):\n",
        "    content = re.sub(\"[^a-zA-Z]\",\" \", content)\n",
        "    words = content.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stops]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5tyKxaBNefo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = pd.DataFrame(list(map(email_preprocessing, sent_user_dataset.message)))\n",
        "final_data = pd.DataFrame(list(map(content_preprocessing, final_data[['Subject', 'content']].apply(lambda x: ' '.join(x), axis=1))), columns = [\"content\"])\n",
        "final_data = final_data.assign(user_number= sent_user_dataset[\"sender\"].values)\n",
        "final_data = final_data.replace({'user_number': mapping})\n",
        "final_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD_hSRWFFlZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emails_words = final_data.content.apply(lambda x: x.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqTWEcpFzOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(emails_words.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agayjmuw7Rd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv.save_word2vec_format('/content/drive/My Drive/Alda_project/model.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G2zD03K7Rq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "filename = '/content/drive/My Drive/Alda_project/model.bin'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVrWpsDnNeaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = final_data.content.values\n",
        "y_data = final_data.user_number.values\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6FcnjylNeQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting into one hot encoded vectors\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(y_data.reshape(-1, 1))\n",
        "y_data = encoder.transform(y_data.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTbtVnY4NaaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = []\n",
        "max_vec_len = len(model['hi'])\n",
        "max_seq_len = 70\n",
        "max_seq_len, max_vec_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjJpk5-yNeEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get feature vectors of the word2vec model\n",
        "for email in emails_words:\n",
        "  x_arr = []\n",
        "  for word in email[:max_seq_len]:\n",
        "    try:\n",
        "      x_arr.append(model[word])\n",
        "    except:\n",
        "      pass\n",
        "  if max_seq_len - len(x_arr) > 0:\n",
        "    for _ in range(max_seq_len - len(x_arr)):\n",
        "      x_arr.append(np.zeros(shape=(max_vec_len,)))\n",
        "  X_data.append(np.array(x_arr))\n",
        "  if len(X_data)%5000 == 0:\n",
        "    print(\"Next 500 batched finished\")\n",
        "X_data = np.array(X_data)\n",
        "\n",
        "np.save('/content/drive/My Drive/Alda_project/word2vec_data', X_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ20EOetRGWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2) #spliting into training and testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxHapUBQ_VOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM model containing encoder LSTM layers and fully connected layers.\n",
        "class LSTM_Model:\n",
        "    \n",
        "    def __init__(self, enc_seq_length, enc_unique_states, output_states, enc_layers=1, \n",
        "                 dense__prev_layers_neurons=[], lstm_units = 256, \n",
        "                 bidirectional=False, dropout=0, recurrent_dropout=0, bias_regularizer=None, \n",
        "                 kernel_regularizer=None, activity_regularizer=None):\n",
        "        self.enc_seq_length = enc_seq_length\n",
        "        self.enc_unique_states = enc_unique_states\n",
        "        self.enc_layers = enc_layers\n",
        "        self.output_states = output_states\n",
        "        self.dense__prev_layers_neurons = dense__prev_layers_neurons\n",
        "        self.lstm_units = lstm_units\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = dropout\n",
        "        self.recurrent_dropout = recurrent_dropout\n",
        "        self.bias_regularizer = bias_regularizer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.activity_regularizer = activity_regularizer\n",
        "        self.dense__prev_layers_neurons.append(self.output_states)\n",
        "        \n",
        "    def getModel(self):\n",
        "        \n",
        "        self.encoder_inputs = Input(shape=(None, self.enc_unique_states), name='encoder_inputs')\n",
        "        \n",
        "        self.encoder = []\n",
        "        self.encoder_outputs = []\n",
        "        \n",
        "        # Add encoder layers \n",
        "        for i in range(self.enc_layers-1):\n",
        "            self.encoder.append(LSTM(self.lstm_units, \n",
        "                                     return_sequences=True, \n",
        "                                     recurrent_dropout=self.recurrent_dropout, \n",
        "                                     dropout = self.dropout, \n",
        "                                     bias_regularizer = self.bias_regularizer, \n",
        "                                     activity_regularizer = self.activity_regularizer, \n",
        "                                     kernel_regularizer=self.kernel_regularizer, \n",
        "                                     name=\"encoder\"+str(i+1)))\n",
        "            # Wrap Bidirectional layer if bidirectional is True\n",
        "            if self.bidirectional:\n",
        "                self.encoder[i] = Bidirectional(self.encoder[i])\n",
        "            \n",
        "        self.encoder.append(LSTM(self.lstm_units,  \n",
        "                                 recurrent_dropout=self.recurrent_dropout, \n",
        "                                 dropout = self.dropout, \n",
        "                                 bias_regularizer = self.bias_regularizer, \n",
        "                                 activity_regularizer = self.activity_regularizer, \n",
        "                                 kernel_regularizer=self.kernel_regularizer, \n",
        "                                 name=\"encoder\"+str(self.enc_layers)))\n",
        "        if self.bidirectional:\n",
        "                self.encoder[self.enc_layers-1] = Bidirectional(self.encoder[self.enc_layers-1])\n",
        "        \n",
        "        # Get encoder outputs for each encoder layer\n",
        "        for i in range(self.enc_layers):\n",
        "            if i==0:\n",
        "                self.encoder_outputs.append(self.encoder[i]((self.encoder_inputs)))\n",
        "            else:\n",
        "                self.encoder_outputs.append(self.encoder[i](self.encoder_outputs[i-1]))\n",
        "        \n",
        "        self.decoder_dense = []\n",
        "        self.dense_outputs = []\n",
        "        self.dense_layers = len(self.dense__prev_layers_neurons)\n",
        "        \n",
        "        # Add fully connected layers\n",
        "        for i in range(self.dense_layers):\n",
        "            if i < self.dense_layers-1:\n",
        "                self.decoder_dense.append(Dense(self.dense__prev_layers_neurons[i], \n",
        "                                                bias_regularizer = self.bias_regularizer, \n",
        "                                                activity_regularizer = self.activity_regularizer, \n",
        "                                                activation='relu', name=\"output_layer\"+str(i+1)))\n",
        "            else:\n",
        "                self.decoder_dense.append(Dense(self.dense__prev_layers_neurons[i], \n",
        "                                                bias_regularizer = self.bias_regularizer, \n",
        "                                                activity_regularizer = self.activity_regularizer, \n",
        "                                                activation='softmax', name=\"softmax\"))                \n",
        "            \n",
        "        # Get outputs of each fully connected layer\n",
        "        for i in range(self.dense_layers):\n",
        "            if i==0:\n",
        "                self.dense_outputs.append(self.decoder_dense[i](self.encoder_outputs[self.enc_layers-1]))\n",
        "            else:\n",
        "                self.dense_outputs.append(self.decoder_dense[i](self.dense_outputs[i-1]))\n",
        "        \n",
        "\n",
        "        self.model = Model(self.encoder_inputs, self.dense_outputs[self.dense_layers-1])\n",
        "        \n",
        "        return self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPhJmGAVBjwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_seq_length = max_seq_len\n",
        "enc_unique_states = max_vec_len\n",
        "output_states = len(encoder.get_feature_names())\n",
        "model = LSTM_Model(enc_seq_length, \n",
        "              enc_unique_states,\n",
        "              output_states,\n",
        "              enc_layers=2,\n",
        "              lstm_units = 128,\n",
        "              dense__prev_layers_neurons=[64],\n",
        "              dropout = 0.3).getModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgREhoU40r1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unCS0gZscAZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REN2YGM0bw4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = keras.callbacks.ModelCheckpoint(\"/content/drive/My Drive/Alda_project/weights_lstm/dense.{epoch:02d}-{val_loss:.2f}.hdf5\", \n",
        "                                       monitor='accuracy',\n",
        "                                       verbose=1, \n",
        "                                       save_best_only=True, \n",
        "                                       mode='max')\n",
        "early_stopping_monitor = keras.callbacks.EarlyStopping(patience=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRUjtF0r06vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_split=0.1, callbacks=[early_stopping_monitor, checkpointer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjcfRWLAcQH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtFcP15NRLWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test.argmax(axis=1), preds.argmax(axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}